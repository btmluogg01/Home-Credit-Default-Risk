{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Essay_Final 1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tZgdaQBzgBM"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "import time\n",
        "from contextlib import contextmanager\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "!pip install xverse\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from scipy.stats import randint\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import  precision_recall_curve, roc_auc_score, confusion_matrix, accuracy_score, recall_score, precision_score, f1_score,auc, roc_curve, plot_confusion_matrix, classification_report"
      ],
      "metadata": {
        "id": "zj4tBwOcz0Yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encoder(df, nan_as_category = True):\n",
        "    original_columns = list(df.columns)\n",
        "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
        "    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n",
        "    new_columns = [c for c in df.columns if c not in original_columns]\n",
        "    return df, new_columns\n",
        "\n",
        "def missing_data(df):\n",
        "  x = df.isnull().sum()\n",
        "  count = len(df)\n",
        "  x = pd.DataFrame(x, columns = ['number_null'])\n",
        "  x['percent'] = round((x['number_null']/count)*100,2)\n",
        "  return x\n",
        "\n",
        "def delete_null_over_p(df, p): \n",
        "  x = missing_data(df)\n",
        "  for i, j in zip(x['percent'], x.index): \n",
        "    if i > p:\n",
        "      del df[j] \n",
        "  return df \n",
        "\n",
        "def corr(df):\n",
        "    corrmat1= df.corr().abs()\n",
        "    upper_tri = corrmat1.where(np.triu(np.ones(corrmat1.shape),k=1).astype(np.bool))\n",
        "    to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.8)]\n",
        "    for i in to_drop:\n",
        "        del df[i]\n",
        "    return df\n",
        "\n",
        "#resampling\n",
        "# umbalanced data\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "def over_sampling(X, y):\n",
        "  ros = RandomOverSampler(random_state=25)\n",
        "  X_ros, y_ros = ros.fit_resample(X, y)\n",
        "  return X_ros, y_ros \n",
        "def under_sampling(X, y):\n",
        "  rus = RandomUnderSampler(random_state = 42, replacement = True)\n",
        "  X_rus, y_rus = rus.fit_resample(X, y)\n",
        "  return X_rus, y_rus\n",
        "def smothing(X, y):\n",
        "  smote = SMOTE()\n",
        "  X_smote, y_smote = smote.fit_resample(X, y)\n",
        "  return X_smote, y_smote\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "def split_data(X,y):\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    return X_train, X_val, y_train, y_val \n",
        "\n",
        "def metrics(model, X_val, y_val):\n",
        "  y_pred = model.predict(X_val)\n",
        "  ypred_prob = model.predict_proba(X_val)[:,1]\n",
        "  target_names = [\"Class 0\", \"Class 1\"]\n",
        "  print(\"AUC\",roc_auc_score(y_val, ypred_prob))\n",
        "  print(classification_report(y_val, y_pred, target_names=target_names))\n",
        "  return None\n",
        "\n",
        "##AUC - ROC\n",
        "def plot_AUC( y_val, ypred_prob):\n",
        "    from sklearn import metrics\n",
        "    fig, (ax, ax1) = plt.subplots(nrows = 1, ncols = 2, figsize = (15,5))\n",
        "    \n",
        "    fpr, tpr, threshold = metrics.roc_curve(y_val, ypred_prob)\n",
        "    roc_auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "    ax.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "    ax.plot([0, 1], [0, 1],'r--')\n",
        "    ax.set_title('Receiver Operating Characteristic ',fontsize=10)\n",
        "    ax.set_ylabel('True Positive Rate',fontsize=20)\n",
        "    ax.set_xlabel('False Positive Rate',fontsize=15)\n",
        "    ax.legend(loc = 'lower right', prop={'size': 16})\n",
        "    plt.subplots_adjust(wspace=1)"
      ],
      "metadata": {
        "id": "UhGaxRfmz4XT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess application_train.csv and application_test.csv\n",
        "def application_train_test(num_rows = None, nan_as_category = False):\n",
        "    # Read data and merge\n",
        "    df = pd.read_csv('/content/drive/MyDrive/Essay/Data/application_train.csv.zip', nrows= num_rows)\n",
        "    test_df = pd.read_csv('/content/drive/MyDrive/Essay/Data/application_test.csv.zip', nrows= num_rows)\n",
        "    print(\"Train samples: {}, test samples: {}\".format(len(df), len(test_df)))\n",
        "    df = df.append(test_df).reset_index()\n",
        "    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)\n",
        "    df = df[df['CODE_GENDER'] != 'XNA']\n",
        "    \n",
        "    # Categorical features with Binary encode (0 or 1; two categories)\n",
        "    for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n",
        "        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n",
        "    # Categorical features with One-Hot encode\n",
        "    df, cat_cols = one_hot_encoder(df, nan_as_category)\n",
        "    \n",
        "    # NaN values for DAYS_EMPLOYED: 365.243 -> nan\n",
        "    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace= True)\n",
        "    # Some simple new features (percentages)\n",
        "    df['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
        "    df['INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']\n",
        "    df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n",
        "    df['ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
        "    df['PAYMENT_RATE'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']\n",
        "    del test_df\n",
        "    gc.collect()\n",
        "    return df\n",
        "\n",
        "def bureau_and_balance(num_rows = None, nan_as_category = True):\n",
        "    bureau = pd.read_csv('/content/drive/MyDrive/Essay/Data/bureau.csv.zip', nrows = num_rows)\n",
        "    bb = pd.read_csv('/content/drive/MyDrive/Essay/Data/bureau_balance.csv.zip', nrows = num_rows)\n",
        "    bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n",
        "    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category)\n",
        "    \n",
        "    # Bureau balance: Perform aggregations and merge with bureau.csv\n",
        "    bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}\n",
        "    for col in bb_cat:\n",
        "        bb_aggregations[col] = ['mean']\n",
        "    bb_agg = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n",
        "    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n",
        "    bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n",
        "    bureau.drop(['SK_ID_BUREAU'], axis=1, inplace= True)\n",
        "    del bb, bb_agg\n",
        "    gc.collect()\n",
        "\n",
        "    # Bureau and bureau_balance numeric features\n",
        "    num_aggregations = {\n",
        "        'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],\n",
        "        'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n",
        "        'DAYS_CREDIT_UPDATE': ['mean'],\n",
        "        'CREDIT_DAY_OVERDUE': ['max', 'mean'],\n",
        "        'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n",
        "        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
        "        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n",
        "        'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
        "        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n",
        "        'AMT_ANNUITY': ['max', 'mean'],\n",
        "        'CNT_CREDIT_PROLONG': ['sum'],\n",
        "        'MONTHS_BALANCE_MIN': ['min'],\n",
        "        'MONTHS_BALANCE_MAX': ['max'],\n",
        "        'MONTHS_BALANCE_SIZE': ['mean', 'sum']\n",
        "    }\n",
        "    # Bureau and bureau_balance categorical features\n",
        "    cat_aggregations = {}\n",
        "    for cat in bureau_cat: cat_aggregations[cat] = ['mean']\n",
        "    for cat in bb_cat: cat_aggregations[cat + \"_MEAN\"] = ['mean']\n",
        "    bureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
        "    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n",
        "    # Bureau: Active credits - using only numerical aggregations\n",
        "    active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n",
        "    active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n",
        "    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n",
        "    bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n",
        "    del active, active_agg\n",
        "    gc.collect()\n",
        "    # Bureau: Closed credits - using only numerical aggregations\n",
        "    closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n",
        "    closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n",
        "    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n",
        "    bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n",
        "    del closed, closed_agg, bureau\n",
        "    gc.collect()\n",
        "    return bureau_agg\n",
        "\n",
        "def previous_applications(num_rows = None, nan_as_category = True):\n",
        "    prev = pd.read_csv('/content/drive/MyDrive/Essay/Data/previous_application.csv.zip', nrows = num_rows)\n",
        "    prev, cat_cols = one_hot_encoder(prev, nan_as_category= True)\n",
        "    # Days 365.243 values -> nan\n",
        "    prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n",
        "    prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n",
        "    prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n",
        "    prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n",
        "    prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n",
        "    # Add feature: value ask / value received percentage\n",
        "    prev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n",
        "    # Previous applications numeric features\n",
        "    num_aggregations = {\n",
        "        'AMT_ANNUITY': ['min', 'max', 'mean'],\n",
        "        'AMT_APPLICATION': ['min', 'max', 'mean'],\n",
        "        'AMT_CREDIT': ['min', 'max', 'mean'],\n",
        "        'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n",
        "        'AMT_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
        "        'AMT_GOODS_PRICE': ['min', 'max', 'mean'],\n",
        "        'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n",
        "        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
        "        'DAYS_DECISION': ['min', 'max', 'mean'],\n",
        "        'CNT_PAYMENT': ['mean', 'sum'],\n",
        "    }\n",
        "\n",
        "    # Previous applications categorical features\n",
        "    cat_aggregations = {}\n",
        "    for cat in cat_cols:\n",
        "        cat_aggregations[cat] = ['mean']\n",
        "    \n",
        "    prev_agg = prev.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
        "    prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n",
        "    # Previous Applications: Approved Applications - only numerical features\n",
        "    approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n",
        "    approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\n",
        "    approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n",
        "    prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR')\n",
        "    # Previous Applications: Refused Applications - only numerical features\n",
        "    refused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n",
        "    refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\n",
        "    refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n",
        "    prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n",
        "    del refused, refused_agg, approved, approved_agg, prev\n",
        "    gc.collect()\n",
        "    return prev_agg\n",
        "\n",
        "\n",
        "# Preprocess POS_CASH_balance.csv\n",
        "def pos_cash(num_rows = None, nan_as_category = True):\n",
        "    pos = pd.read_csv('/content/drive/MyDrive/Essay/Data/POS_CASH_balance.csv.zip', nrows = num_rows)\n",
        "    pos, cat_cols = one_hot_encoder(pos, nan_as_category= True)\n",
        "    # Features\n",
        "    aggregations = {\n",
        "        'MONTHS_BALANCE': ['max', 'mean', 'size'],\n",
        "        'SK_DPD': ['max', 'mean'],\n",
        "        'SK_DPD_DEF': ['max', 'mean']\n",
        "    }\n",
        "    for cat in cat_cols:\n",
        "        aggregations[cat] = ['mean']\n",
        "    \n",
        "    pos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\n",
        "    pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n",
        "    # Count pos cash accounts\n",
        "    pos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\n",
        "    del pos\n",
        "    gc.collect()\n",
        "    return pos_agg\n",
        "\n",
        "# Preprocess installments_payments.csv\n",
        "def installments_payments(num_rows = None, nan_as_category = True):\n",
        "    ins = pd.read_csv('/content/drive/MyDrive/Essay/Data/installments_payments.csv.zip', nrows = num_rows)\n",
        "    ins, cat_cols = one_hot_encoder(ins, nan_as_category= True)\n",
        "    # Percentage and difference paid in each installment (amount paid and installment value)\n",
        "    ins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\n",
        "    ins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n",
        "    # Days past due and days before due (no negative values)\n",
        "    ins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\n",
        "    ins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\n",
        "    ins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0)\n",
        "    ins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0)\n",
        "    # Features: Perform aggregations\n",
        "    aggregations = {\n",
        "        'NUM_INSTALMENT_VERSION': ['nunique'],\n",
        "        'DPD': ['max', 'mean', 'sum'],\n",
        "        'DBD': ['max', 'mean', 'sum'],\n",
        "        'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'],\n",
        "        'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n",
        "        'AMT_INSTALMENT': ['max', 'mean', 'sum'],\n",
        "        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
        "        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum']\n",
        "    }\n",
        "\n",
        "    for cat in cat_cols:\n",
        "        aggregations[cat] = ['mean']\n",
        "    ins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\n",
        "    ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n",
        "    # Count installments accounts\n",
        "    ins_agg['INSTAL_COUNT'] = ins.groupby('SK_ID_CURR').size()\n",
        "    del ins\n",
        "    gc.collect()\n",
        "    return ins_agg\n",
        "\n",
        "\n",
        "# Preprocess credit_card_balance.csv\n",
        "def credit_card_balance(num_rows = None, nan_as_category = True):\n",
        "    cc = pd.read_csv('/content/drive/MyDrive/Essay/Data/credit_card_balance.csv.zip', nrows = num_rows)\n",
        "    cc, cat_cols = one_hot_encoder(cc, nan_as_category= True)\n",
        "    # General aggregations\n",
        "    cc.drop(['SK_ID_PREV'], axis= 1, inplace = True)\n",
        "    cc_agg = cc.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum', 'var'])\n",
        "    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n",
        "    # Count credit card lines\n",
        "    cc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\n",
        "    del cc\n",
        "    gc.collect()\n",
        "    return cc_agg"
      ],
      "metadata": {
        "id": "4aKTjlXZ0JV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = application_train_test(num_rows = None, nan_as_category = False)\n",
        "bureau = bureau_and_balance(num_rows = None, nan_as_category = True)\n",
        "prev = previous_applications(num_rows = None, nan_as_category = True)\n",
        "pos = pos_cash(num_rows = None, nan_as_category = True)\n",
        "ins = installments_payments(num_rows = None, nan_as_category = True)\n",
        "cc = credit_card_balance(num_rows = None, nan_as_category = True)"
      ],
      "metadata": {
        "id": "6z8lVvfY0h3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVDyW8Tg9K5L"
      },
      "outputs": [],
      "source": [
        "df = df.join(bureau, how='left', on='SK_ID_CURR')\n",
        "df = df.join(prev, how='left', on='SK_ID_CURR')\n",
        "df = df.join(pos, how='left', on='SK_ID_CURR')\n",
        "df = df.join(ins, how='left', on='SK_ID_CURR')\n",
        "df = df.join(cc, how='left', on='SK_ID_CURR')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "missing_data(df)\n",
        "df = delete_null_over_p(df, p = 75)\n",
        "corr(df)\n",
        "for i in df.columns:\n",
        " df[i].fillna(df[i].mode()[0], inplace=True)\n",
        "print(df)"
      ],
      "metadata": {
        "id": "G5uxnA0K0l9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df.copy()\n",
        "df2 = df.copy()\n",
        "df3 = df.copy()"
      ],
      "metadata": {
        "id": "2H3b-dBT42G-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method 1 : WOE - IV => Logistic"
      ],
      "metadata": {
        "id": "bzw6IDSD1ry5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 1 : WOE - IV => Logistic\n",
        "from xverse.transformer import WOE\n",
        "def WOE_IV():\n",
        "  X = df1.drop('TARGET', axis = 1)\n",
        "  y = df1.TARGET\n",
        "  clf = WOE()\n",
        "  clf.fit(X, y)\n",
        "  clf.woe_df\n",
        "  iv = clf.iv_df\n",
        "  return iv\n",
        "\n",
        "def drop_iv(iv, p):\n",
        "  iv_1 = iv[iv['Information_Value']>p]\n",
        "  return iv_1, iv_1['Variable_Name']\n",
        "\n",
        "def X_y(iv_1):\n",
        "  X=df1[[col for col in iv_1['Variable_Name']]]\n",
        "  clf = WOE()\n",
        "  X = clf.transform(X)\n",
        "  y = df1.TARGET\n",
        "  return X,y\n",
        "\n",
        "## so sánh model log và model đã được resampling\n",
        "def log(X_train,y_train,X_val, y_val):\n",
        "  log = LogisticRegression()\n",
        "  model = log.fit(X_train, y_train)\n",
        "  y_predict = log.predict(X_val)\n",
        "  a = metrics(model,X_val,y_val)\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "wCjXi5KD00HC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iv = WOE_IV()\n",
        "iv_1, iv_1['Variable_Name'] = drop_iv(iv, 0.02)\n",
        "X, y = X_y(iv_1)\n",
        "X_train, X_val, y_train, y_val = split_data(X,y)\n",
        "#Sampling\n",
        "X_ros, y_ros = over_sampling(X_train, y_train)\n",
        "X_rus, y_rus = under_sampling(X_train, y_train)\n",
        "X_smooth, y_smooth = smothing(X_train, y_train)\n",
        "#\n",
        "log(X_train,y_train,X_val, y_val)\n",
        "#Sampling\n",
        "print(\"Oversampling:\")\n",
        "model1 = log(X_ros, y_ros, X_val, y_val)\n",
        "print(\"\\nUndersampling:\")\n",
        "model2 = log(X_rus, y_rus, X_val, y_val)\n",
        "print(\"\\nSmote: \")\n",
        "model3 = log(X_smooth, y_smooth, X_val, y_val)"
      ],
      "metadata": {
        "id": "Qrp3NDTh4BfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method 2: Machine Learning"
      ],
      "metadata": {
        "id": "BxyupBHB5UUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "def scale(X_train, X_val):\n",
        "    sc_X = StandardScaler()\n",
        "    X_train = sc_X.fit_transform(X_train)\n",
        "    X_val = sc_X.transform(X_val)\n",
        "    return X_train, X_val\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "def scale_minmax(X_train, X_val):\n",
        "  scaler_minmax = MinMaxScaler()\n",
        "  X_train = scaler_minmax.fit_transform(X_train)\n",
        "  X_val = scaler_minmax.transform(X_val)\n",
        "  return X_train, X_val\n",
        "\n",
        "## Logistic Regression.\n",
        "def log(X_train,y_train,X_val, y_val):\n",
        "  log = LogisticRegression()\n",
        "  model = log.fit(X_train, y_train)\n",
        "  y_predict = log.predict(X_val)\n",
        "  a = metrics(model,X_val,y_val)\n",
        "  return model\n",
        "\n",
        "## Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "def model_random_forest(X_train, y_train, X_val, y_val): \n",
        "  clf = RandomForestClassifier(criterion=\"gini\",\\\n",
        "                               random_state = 25,\\\n",
        "                               min_samples_leaf=5)\n",
        "  model = clf.fit(X_train, y_train)\n",
        "  y_predict = clf.predict(X_val)\n",
        "  a = metrics(model,X_val,y_val)\n",
        "  return model\n",
        "\n",
        "#lightgbm\n",
        "def model_LGBM(X_train, y_train, X_val, y_val): \n",
        "  clf = lgb.LGBMClassifier(boosting_type = 'goss',\n",
        "            nthread=4,\n",
        "            n_estimators=10000,\n",
        "            learning_rate=0.005134,\n",
        "            num_leaves=54,\n",
        "            colsample_bytree=0.508716,\n",
        "            subsample=1,\n",
        "            max_depth=10,\n",
        "            reg_alpha=0.436193,\n",
        "            reg_lambda=0.479169,\n",
        "            min_split_gain=0.024766,\n",
        "            min_child_weight=40,\n",
        "            silent=-1,\n",
        "            verbose=-1,\n",
        "            is_unbalance=False)\n",
        "  model = clf.fit(X_train, y_train)\n",
        "  a = metrics(model,X_val,y_val)\n",
        "  return model\n",
        "\n",
        "#Hyperparameters tuning with randomsearchcv\n",
        "def randomsearchcv_lgbm(X_train, y_train, X_val, y_val):\n",
        "  clf = lgb.LGBMClassifier()\n",
        "  param_dist = { \"learning_rate\": np.linspace(0,0.2,5),\n",
        "               \"max_depth\": randint(3, 10),\n",
        "                \"min_split_gain\": np.linspace(1, 10, 1),\n",
        "                \"num_iterations\": randint(100, 10000),\n",
        "                \"min_data_in_leaf\": randint(3, 10),\n",
        "                \"min_gain_to_split\": randint(1, 10),\n",
        "                \"max_bin\": randint(10, 100)}\n",
        "               \n",
        "  model = RandomizedSearchCV(clf , param_dist, scoring='accuracy', cv =5)\n",
        "  model.fit(X_train,y_train)\n",
        "  best_params = model.best_estimator_\n",
        "  y_predict = best_params.predict(X_val)\n",
        "  y_predict_proba = best_params.predict_proba(X_val)[:, 1]\n",
        "  a = metrics(model,X_val,y_val)\n",
        "  return model"
      ],
      "metadata": {
        "id": "AZ1SS4qR5UDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df2.replace((np.inf, -np.inf, np.nan), 0).reset_index(drop=True)\n",
        "#Split\n",
        "y = df2.TARGET\n",
        "X = df2.drop('TARGET', axis = 1)\n",
        "X_train, X_val, y_train, y_val = split_data(X,y)\n",
        "scale(X_train, X_val)\n",
        "\n",
        "#Sampling\n",
        "X_ros, y_ros = over_sampling(X_train, y_train)\n",
        "X_rus, y_rus = under_sampling(X_train, y_train)\n",
        "X_smooth, y_smooth = smothing(X_train, y_train)"
      ],
      "metadata": {
        "id": "oXix1Bmz50p_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Logistic\n",
        "print(\"Logistic:\")\n",
        "log(X_train,y_train,X_val, y_val)\n",
        "print(\"Oversampling_log:\")\n",
        "model1 = log(X_ros, y_ros, X_val, y_val)\n",
        "print(\"\\nUndersampling_log:\")\n",
        "model2 = log(X_rus, y_rus, X_val, y_val)\n",
        "print(\"\\nSmote_log: \")\n",
        "model3 = log(X_smooth, y_smooth, X_val, y_val)\n",
        "\n",
        "#Random Forest\n",
        "print(\"\\nRandom Forest:\")\n",
        "rf(X_train,y_train,X_val, y_val)\n",
        "print(\"\\nOversampling_rf:\")\n",
        "model1 = model_random_forest(X_ros, y_ros, X_val, y_val)\n",
        "print(\"\\nUndersampling_rf:\")\n",
        "model2 = model_random_forest(X_rus, y_rus, X_val, y_val)\n",
        "print(\"\\nSmote_rf: \")\n",
        "model3 = model_random_forest(X_smooth, y_smooth, X_val, y_val)\n",
        "\n",
        "#LightGBM\n",
        "print(\"\\nLightGBM:\")\n",
        "model_LGBM(X_train,y_train,X_val, y_val)\n",
        "print(\"\\nOversampling LightGBM:\")\n",
        "model1 = model_LGBM(X_ros, y_ros, X_val, y_val)\n",
        "print(\"\\nUndersampling LightGBM:\")\n",
        "model2 = model_LGBM(X_rus, y_rus, X_val, y_val)\n",
        "print(\"\\nSmote LightGBM: \")\n",
        "model3 = model_LGBM(X_smooth, y_smooth, X_val, y_val)\n",
        "\n",
        "#\n",
        "print('\\nHyperparameter with LightGBM:')\n",
        "randomsearchcv_lgbm(X_train,y_train,X_val, y_val)\n",
        "print(\"\\nOversampling Hyper LightGBM:\")\n",
        "model1 = randomsearchcv_lgbm(X_ros, y_ros, X_val, y_val)\n",
        "print(\"\\nUndersampling Hyper LightGBM:\")\n",
        "model2 = randomsearchcv_lgbm(X_rus, y_rus, X_val, y_val)\n",
        "print(\"\\nSmote Hyper LightGBM: \")\n",
        "model3 = randomsearchcv_lgbm(X_smooth, y_smooth, X_val, y_val)"
      ],
      "metadata": {
        "id": "ZHjQ3IPk6oIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Method 3: K-Fold => LightGBM"
      ],
      "metadata": {
        "id": "gMweFN_C8-pV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def kfold_lightgbm(df3, num_folds, stratified = False, debug= False):\n",
        "    # Divide in training/validation and test data\n",
        "    train_df = df[df['TARGET'].notnull()]\n",
        "    test_df = df[df['TARGET'].isnull()]\n",
        "    print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n",
        "    del df\n",
        "    gc.collect()\n",
        "    # Cross validation model\n",
        "    if stratified:\n",
        "        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=1001)\n",
        "    else:\n",
        "        folds = KFold(n_splits= num_folds, shuffle=True, random_state=1001)\n",
        "    # Create arrays and dataframes to store results\n",
        "    oof_preds = np.zeros(train_df.shape[0])\n",
        "    sub_preds = np.zeros(test_df.shape[0])\n",
        "    feature_importance_df = pd.DataFrame()\n",
        "    feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n",
        "    \n",
        "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n",
        "        train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n",
        "        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n",
        "\n",
        "        # LightGBM parameters found by Bayesian optimization\n",
        "        clf = LGBMClassifier(\n",
        "            nthread=4,\n",
        "            n_estimators=10000,\n",
        "            learning_rate=0.02,\n",
        "            num_leaves=34,\n",
        "            colsample_bytree=0.9497036,\n",
        "            subsample=0.8715623,\n",
        "            max_depth=8,\n",
        "            reg_alpha=0.041545473,\n",
        "            reg_lambda=0.0735294,\n",
        "            min_split_gain=0.0222415,\n",
        "            min_child_weight=39.3259775,\n",
        "            silent=-1,\n",
        "            verbose=-1, )\n",
        "\n",
        "        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n",
        "            eval_metric= 'auc', verbose= 200, early_stopping_rounds= 200)\n",
        "\n",
        "        oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n",
        "        sub_preds += clf.predict_proba(test_df[feats], num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits\n",
        "\n",
        "        fold_importance_df = pd.DataFrame()\n",
        "        fold_importance_df[\"feature\"] = feats\n",
        "        fold_importance_df[\"importance\"] = clf.feature_importances_\n",
        "        fold_importance_df[\"fold\"] = n_fold + 1\n",
        "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
        "        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n",
        "        del clf, train_x, train_y, valid_x, valid_y\n",
        "        gc.collect()\n",
        "\n",
        "    print('Full AUC score %.6f' % roc_auc_score(train_df['TARGET'], oof_preds))\n",
        "    return None\n",
        "\n",
        "feat_importance = kfold_lightgbm(df, num_folds= 10, stratified= False)"
      ],
      "metadata": {
        "id": "x3A59soE895h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}